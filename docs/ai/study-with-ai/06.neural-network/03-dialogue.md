# 03. Dialogue - 표현력, 학습, 그리고 일반화의 긴장

## **1️⃣ 문제 제기: 선형 모델의 한계**

Week 5까지 우리는:

- 손실 함수
- 최적화
- 일반화
- 정규화

를 배웠다.

하지만 전제가 있었다.

> 모델이 표현할 수 있는 함수가 충분하다는 전제.

그러나 선형 모델은:

- 결정 경계가 직선(초평면)
- XOR 같은 문제 해결 불가

즉,

> 문제는 학습이 아니라 **표현력의 부족**이었다.

## **2️⃣ 해결: 비선형 도입**

선형을 아무리 많이 쌓아도 여전히 선형이다.

그래서 필요한 것:

> 비선형 활성화 함수

비선형이 들어가면:

- 입력이 새로운 표현 공간으로 변환된다.
- 그 공간에서는 선형 분리가 가능해진다.

핵심 통찰:

> 신경망은 “경계를 직접 그리는 모델”이 아니라
> “입력을 재표현하는 모델”이다.

## **3️⃣ 깊이의 의미**

신경망 구조:

```
y = W_3 \sigma(W_2 \sigma(W_1 x))
```

이는 단순 계산이 아니라:

> 함수의 합성(composition)

깊이가 증가하면:

- 표현이 누적
- 공간 분할이 조합적으로 증가
- 계층적 구조 표현 가능

이미지/언어는:

- 단순 패턴 → 중간 개념 → 고차 개념

같은 계층 구조를 가지므로

> 깊은 모델이 구조적으로 더 적합하다.

## **4️⃣ 그런데 학습은 어떻게 가능한가?**

표현력만 있다고 끝이 아니다.

### **Backpropagation**

- 출력 오차 계산
- 오차를 뒤로 전달
- 각 층의 영향도(gradient) 계산

### **경사하강법**

- Loss를 줄이는 방향으로 조금씩 이동

핵심:

> 신경망은 미분 가능하기 때문에 학습 가능하다.

## **5️⃣ 깊이의 부작용**

합성 함수의 미분은 “곱”으로 전달된다.

그래서:

- 작은 값 반복 → Vanishing Gradient
- 큰 값 반복 → Exploding Gradient

이 문제 때문에 초기 신경망은 깊게 학습하기 어려웠다.

### **해결책**

- ReLU (양수 구간 미분 1)
- 적절한 초기화
- BatchNorm

이 기술들 덕분에 딥러닝이 가능해졌다.

## **6️⃣ 표현력 vs 일반화**

여기서 Week 5와 다시 연결된다.

Week 5에서 배운 것:

> 자유도가 크면 Variance 증가 → 과적합

그런데 딥러닝은:

- 파라미터가 매우 많음
- 표현력이 매우 큼

그럼 과적합이어야 하지 않나?

## **7️⃣ 현대 딥러닝의 특징**

### **암묵적 정규화**

SGD는:

- 모든 해 중 아무 해나 선택하지 않는다.
- 특정 구조(부드럽고 안정적인 해)를 선호하는 경향

### **명시적 정규화**

- L2
- Dropout
- BatchNorm

이들이 함께 작동한다.

## **8️⃣ Double Descent**

고전적 이론:

- 모델 복잡도 증가 → 성능 하락

현대 관측:

- 복잡도 증가 → 한 번 나빠졌다가 → 다시 좋아짐

즉,

> 과매개변수화 + SGD가 새로운 일반화 구조를 만든다.

## **🎯 Week 6 전체 맥락**

1. 선형 모델은 표현력에 한계가 있다.
2. 비선형은 표현 공간을 재구성한다.
3. 깊이는 계층적 표현을 가능하게 한다.
4. Backprop으로 학습이 가능하다.
5. 깊이는 Gradient 문제를 만든다.
6. 현대 기술로 이를 해결했다.
7. 표현력과 일반화는 긴장 관계에 있다.
8. 하지만 학습 과정이 해를 제한한다.

## **🔥 Week 6의 본질**

> 신경망은 표현력을 극단적으로 확장했으며,
> 그 표현을 미분 기반 최적화로 학습 가능하게 만들었다.
> 그러나 표현력 증가는 일반화와 긴장 관계에 있고,
> 현대 딥러닝은 학습 알고리즘과 정규화로 이를 조절한다.
