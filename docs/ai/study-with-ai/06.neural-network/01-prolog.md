# 01. Prolog - 신경망은 왜 필요했는가?

- 데이터로 규칙을 학습한다 ✔
- 손실을 줄이며 최적화한다 ✔
- 일반화를 고민한다 ✔
- 정규화로 과적합을 줄인다 ✔

그런데…

> **아무리 정규화를 잘해도**
> **어떤 문제들은 애초에 잘 표현되지 않는다.**

왜 그럴까?

## **1️⃣ 핵심 문제: 표현력(Representation)**

지금까지 우리가 암묵적으로 사용한 모델은:

- 선형 모델 (Linear Model)

형태는 단순하다:

```
y = w_1x_1 + w_2x_2 + ... + b
```

즉,

> 입력들의 “가중합”
> 이다.

## **2️⃣ 선형 모델의 한계**

선형 모델이 잘하는 것:

- 직선 / 평면으로 구분 가능한 문제
- 입력과 출력이 비교적 단순하게 연결된 경우

하지만 이런 문제는?

- XOR 문제
- 이미지 분류
- 언어 이해

👉 선형 결합으로는 표현이 안 된다.

아무리:

- 데이터를 많이 줘도
- 정규화를 조정해도
- 최적화를 잘해도

**표현 자체가 안 된다.**

이건 일반화 문제가 아니라,

> **표현력의 한계 문제다.**

## **3️⃣ 여기서 사고가 바뀐다**

Week 5까지의 질문은:

> “이 모델을 믿어도 되나?”

Week 6의 질문은:

> “이 모델은 애초에 이 문제를 표현할 수 있나?”

완전히 다른 층위다.

## **4️⃣ 해결 아이디어: 비선형성(Non-linearity)**

핵심 아이디어는 이것이다:

> 단순한 선형 결합을 여러 번 쌓으면
> 더 복잡한 함수를 만들 수 있지 않을까?

그리고 여기서 등장하는 것이:

- 퍼셉트론
- 다층 신경망 (MLP)
- 활성화 함수
