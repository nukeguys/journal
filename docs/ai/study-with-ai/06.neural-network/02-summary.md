# 02. Summary - 표현력과 학습 가능성

## **1️⃣ 출발점: 선형 모델의 한계**

- 선형 모델은 1차 함수만 표현 가능
- 결정 경계는 직선(고차원에서는 초평면)
- XOR처럼 선형 분리 불가능한 문제 존재

핵심:

> 한계는 데이터가 아니라 표현 구조의 한계였다.

## **2️⃣ 비선형의 도입**

- 선형을 여러 번 쌓아도 여전히 선형
- 중간에 비선형 활성화 필요

비선형의 역할:

> 입력을 새로운 표현 공간으로 재구성

그 결과:

- 원래 공간에서는 분리 불가능
- 재표현된 공간에서는 선형 분리 가능

## **3️⃣ 신경망의 본질**

신경망은:

```
y = W_2 \sigma(W_1 x)
```

구조적으로:

- 선형 → 비선형 → 선형
- 함수 합성 구조

핵심:

> 신경망은 경계를 만드는 모델이 아니라
> **함수를 근사하는 모델**

분류와 회귀는 출력 해석 방식 차이일 뿐이다.

## **4️⃣ 깊이와 너비**

- 깊이: 비선형 변환의 반복 → 함수 합성
- 너비: 한 층의 표현 세밀도 증가

핵심:

> 깊이는 계층적·조합적 구조를 표현하는 데 유리

이미지·언어는 계층적 구조를 가지므로
깊은 모델이 효율적이다.

## **5️⃣ 학습은 어떻게 가능한가?**

### **Backpropagation**

- 오차를 뒤로 전파
- 각 층의 “책임” 계산
- 미분(gradient)을 통해 영향도 측정

### **경사하강법**

- Loss가 줄어드는 방향으로 조금씩 이동

핵심:

> 신경망은 미분 가능하기 때문에 학습 가능하다.

## **6️⃣ 깊이의 부작용**

### **Vanishing / Exploding Gradient**

- 미분은 곱으로 전달됨
- 1보다 작은 값이 반복되면 기울기 소실
- 1보다 크면 폭발

### **해결**

- ReLU 활성화
- 초기화 기법
- BatchNorm 등

## **7️⃣ 표현력 vs 일반화**

Week 5와 연결:

- 표현력 증가 → 과적합 위험 증가

하지만 현대 딥러닝에서는:

- 과매개변수화 모델이 오히려 잘 일반화하는 경우 존재
- SGD의 암묵적 정규화
- Dropout, L2, BatchNorm 등과 함께 작동

## **8️⃣ Double Descent**

- 모델 복잡도 증가 → 성능이 한 번 나빠졌다가 다시 좋아질 수 있음
- 고전적 Bias-Variance 곡선과 다른 현대 현상

## **🎯 Week 6 핵심 요약 한 문장**

> 신경망은 비선형 변환을 통해 입력을 재표현하고, 깊이를 통해 함수 합성 구조를 형성하며, Backpropagation을 통해 학습 가능한 강력한 함수 근사기가 된다. 그러나 표현력 증가는 일반화와 긴장 관계를 가지며, 현대 딥러닝은 학습 과정과 정규화를 통해 이를 조절한다.
